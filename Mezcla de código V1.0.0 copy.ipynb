{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenamiento_alternativo(inic_def, n_filas, start, Q, R, m2):\n",
    "    \n",
    "    for c in m2.valores:\n",
    "        c.destroy()\n",
    "\n",
    "    #inic_def= int(entry_inic.get())\n",
    "    goal= int(entry_fin.get()) #META DEL RECORRIDO\n",
    "    episodios= int(entry_iteraciones.get())#NÚMERO DE ITERACIONES QUE TENDRÁ EL ENTRENAMIENTO\n",
    "    gamma= float(entry_gamma.get()) #FACTOR DE APRENDIZAJE\n",
    "    state = random.randint(0,len(Q[0])-1) #ESTADO ALEATORIO DE INICIO DEL ENTRENAMIENTO \n",
    "    \n",
    "    epsilon = float(entry_epsilon.get()) #HAY QUE METERLO POR INTERFAZ DE USUARIO\n",
    "    alpha =  float(entry_alpha.get())  #HAY QUE METERLO POR INTERFAZ DE USUARIO\n",
    "    \n",
    "    #VALIDACION DE EPSILON Y ALPHA\n",
    "\n",
    "\n",
    "    print(\"Estado inicial escogido: \", state)\n",
    "    print(\"Meta escogida: \", goal)\n",
    "    print(\"Gamma: \", gamma)\n",
    "    print(\"Epsilon: \", epsilon)\n",
    "    print(\"Alpha: \", alpha)\n",
    "    print(\"Episodios: \", episodios)\n",
    "\n",
    "    for x in range(episodios):\n",
    "        print(\"Estado actual: \", state)\n",
    "        actions = np.where(R[state]!=-1)[0] #LISTA DE ACCIONES QUE SON POSIBLES\n",
    "        print(\"Posibles acciones: \", actions)\n",
    "        \n",
    "        if(random.uniform(0, 1) < epsilon):\n",
    "            print(\"Se ha ejectuado la Fase 1.\")\n",
    "            next_state = random.choice(actions) #ESCOJO UNA ACCIÓN ALEATORIAMENTE, LA CUAL SERÁ MI PRÓXIMO ESTADO\n",
    "            print(\"Acción escogida al azar: \", next_state)\n",
    "        else:\n",
    "            print(\"Se ha ejectuado la Fase 2.\")\n",
    "            next_state = 0\n",
    "            Q_aux = -1\n",
    "            \n",
    "            for st in actions:                  #EN ESTE BUCLE NOS QUEDAREMOS CON EL PRÓXIMO ESTADO CON MAYOR VALOR EN Q\n",
    "                if(Q[state, st] > Q_aux):            \n",
    "                    Q_aux = Q[state, st]\n",
    "                    next_state = st\n",
    "\n",
    "            print(\"Acción escogida con el máximo Q: \", next_state)\n",
    "\n",
    "        print(\"Peso en R de esa acción: \", R[state, next_state])\n",
    "        next_actions = np.where(R[next_state]!=-1)[0] #LISTA DE ACCIONES QUE SON POSIBLES EN EL PRÓXIMO ESTADO\n",
    "        print(\"Acciones que serán posibles cuando esté en\", next_state, \": \", next_actions)\n",
    "\n",
    "        next_actions_Qvalue = np.ones(len(next_actions), dtype=int)\n",
    "\n",
    "        for y in range(len(next_actions)):            #MEDIANTE ESTE BUCLE, DEJAREMOS PREPARADO UN ARRAY CON LOS VALORES EN Q \n",
    "            next_actions_Qvalue[y] = Q[next_state, next_actions[y]]        #DEL FUTURO ESTADO, ASÍ PODREMOS SACARLE EL MAX\n",
    "\n",
    "        print(\"Peso en Q para esas futuras acciones: \", next_actions_Qvalue)\n",
    "\n",
    "        Q[state, next_state] = R[state, next_state] + gamma * max(next_actions_Qvalue)\n",
    "        state = next_state\n",
    "        epsilon = epsilon * alpha\n",
    "        \n",
    "        print(\"El rendimiento de este episodio de entrenamiento ha sido: \", (Q.sum()/np.max(Q))*100) #RENDIMIENTO\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        if(state==goal):\n",
    "            print(\"Hemos alcanzado la meta antes de terminar las iteraciones.\")\n",
    "            break\n",
    "    \n",
    "    entry_epsilon.delete(0,END)\n",
    "    entry_epsilon.insert(END,epsilon) #ACTUALIZAMOS EL VALOR DE EPSILON EN LA INTERFAZ DE USUARIO\n",
    "    \n",
    "    salida_Q=Label(ventana, text=Q)\n",
    "    salida_Q.grid(column=0,row=21)\n",
    "    m2.valores.append(salida_Q)\n",
    "    print(Q)\n",
    "    \n",
    "    finish_program = Button(ventana, text=\"Recorrer tablero\", command=lambda: recorrer_tablero(inic_def, goal_def, Q, R))\n",
    "    finish_program.grid(column=0, row=n_filas+21)\n",
    "    start.append(finish_program) #LLAMADA A RECORRER EL TABLERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "def iniciar_tablero(n_filas, start,m2):\n",
    "\n",
    "    for c in m2.valores:\n",
    "        c.destroy()\n",
    "    for c in m2.celdas:\n",
    "        c.destroy()\n",
    "    del m1.nueva_lista[:]\n",
    "    \n",
    "    #LECTURA DE LA MATRIZ DE LA INTERFAZ DE USUARIO, ES LA QUE USARÉ\n",
    "    n_columnas= int(entry_columna.get())\n",
    "    lista_tablero=[]\n",
    "    T=[]    #Array de valores de la matriz que son TRAMPA\n",
    "    B=[]    #Array de valores de la matriz que son BONIFICACIONES\n",
    "    for c in m1.celdas:\n",
    "        try:\n",
    "            lista_tablero.append(int(c.get()))  #INTENTO PASAR A ENTERO EL VALOR DE LA CASILLA, SI NO ME DEJA ES PORQUE ES UNA CASILLA TRAMPA O BONUS\n",
    "        except ValueError:\n",
    "            recompensa = str(c.get())\n",
    "            if(recompensa.startswith(\"T\")):             #SI EMPIEZA POR T ES TRAMPA -> (T,6)\n",
    "                T_aux = recompensa.split(\",\")           #AUX ES LISTA DE DOS ELEMENTOS [[T],[6]]\n",
    "                T_aux.remove(\"T\")                       #ELIMINO LA T DE LA LISTA\n",
    "                val=T_aux[0]                            #ME QUEDO CON EL NUMERO\n",
    "                T.append(int(val))\n",
    "                lista_tablero.append(int(val))\n",
    "            else:\n",
    "                B_aux = recompensa.split(\",\")\n",
    "                B_aux.remove(\"B\")\n",
    "                val=B_aux[0]\n",
    "                B.append(int(val))            \n",
    "                lista_tablero.append(int(val))\n",
    "\n",
    "    set_tablero= set(lista_tablero)\n",
    "\n",
    "    inic_def= int(entry_inic.get())\n",
    "    goal_def= int(entry_fin.get())\n",
    "    iteraciones= int(entry_iteraciones.get())\n",
    "    gamma_def= float(entry_gamma.get()) \n",
    "\n",
    "    #VALIDACION DE FORMULARIO#\n",
    "    #VALIDACION INICIO Y GOAL DENTRO DE MATRIZ DE NUMEROS\n",
    "    if(not(goal_def in lista_tablero) or not(inic_def in lista_tablero)):\n",
    "        mb.showerror(\"Error\",\"Posición de Inicio y Objetivo han de ser números que existan en la matriz\")\n",
    "    #MATRIZ DE NUMEROS\n",
    "    if(len(set_tablero) < len(lista_tablero)):\n",
    "        mb.showerror(\"Error\",\"No puede repetir numero en la matriz\")\n",
    "\n",
    "    #VALIDACION DE TRAMPA Y BONUS: NO PUEDE HABER TRAMPA NI BONUS EN INICIO NI FIN\n",
    "    if((goal_def in T) or (goal_def in B) or (inic_def in T) or (inic_def in B)):\n",
    "        mb.showerror(\"Error\",\"Posición de Inicio y Objetivo no deben de ser posiciones de bonus ni trampa\")\n",
    "\n",
    "    #VALIDACION GAMMA EPSILON Y ALFA: ENTRE 0 Y 1\n",
    "    if(not(gamma_def>0 and gamma_def<1)):\n",
    "        mb.showerror(\"Error\",\"Gamma debe de estar entre 0 y 1\")\n",
    "\n",
    "    print(\"T\",T)\n",
    "    print(\"B\",B)\n",
    "    print(\"Lista\",lista_tablero)\n",
    "    print(\"Set\",set_tablero)\n",
    "    \n",
    "    lista_tablero = [lista_tablero[i:i+n_columnas] for i in range(0, len(lista_tablero), n_columnas)]\n",
    "    m1.nueva_lista=lista_tablero\n",
    "\n",
    "    #A CONTINUACIÓN VAMOS A DEFINIR TODAS LAS PROPIEDADES DEL PROBLEMA\n",
    "    tablero_array = [[0,1],[2,3],[4,5]]\n",
    "    #tablero_array = [[0,1,2],[3,4,5],[6,7,8]]\n",
    "    tablero_array = lista_tablero #TABLERO QUE VAMOS A UTILIZAR\n",
    "    tablero = np.array(tablero_array) #AQUÍ LO CONVERTIMOS EN UNA MATRIZ\n",
    "    casillas = len(tablero)*len(tablero[0]) #NÚMERO DE CASILLAS DEL TABLERO\n",
    "    goal = int(goal_def) #META PARA TERMINAR EL ENTRENAMIENTO\n",
    "    Q = np.zeros((casillas, casillas), dtype = int) #MATRIZ DE ENTRENAMIENTO INICIALIZADA A 0\n",
    "    R = np.full((casillas, casillas), -1) #MATRIZ DE MOVIMIENTOS INICIALIZADA A -1\n",
    "\n",
    "    #DECLARACIÓN DE R, LA MATRIZ DE MOVIMIENTOS POSIBLES. UNA POSICIÓN DE ESTA MATRIZ TENDRÁ EL VALOR 0 CUANDO EXISTA CAMINO\n",
    "    #ENTRE X (STATE) E Y (ACTION). SI UNO DE ESOS CAMINOS VÁLIDOS CONDUCE A LA META, EL VALOR PASARÁ A SER 100.\n",
    "    #PARA CAMINOS NO VÁLIDOS (POR NO SER CASILLAS VECINAS), EL VALOR SERÁ -1\n",
    "    #HEMOS CONSIDERADO QUE LA META ES LA ÚNICA CASILLA CON UN CAMINO QUE LA LLEVA DE VUELTA A ELLA MISMA\n",
    "\n",
    "    for casilla in range(casillas): #VAMOS A ITERAR POR TODAS LAS CASILLAS DEL TABLERO\n",
    "        ubicacion = np.where(tablero==casilla) #ALMACENAMOS LAS COORDENADAS DE LA CASILLA DE LA ITERACIÓN ACTUAL\n",
    "\n",
    "        for y in range(casillas):\n",
    "            ubicacion_proxima=np.where(tablero==y) #ALMACENAMOS LAS COORDENADAS DE OTRA CASILLA DEL TABLERO PARA COMPROBAR SU\n",
    "                                                   #CERCANÍA CON LA CASILLA DE LA ITERACIÓN ACTUAL\n",
    "            #COMPROBACIÓN HORIZONTAL Y VERTICAL\n",
    "            if(ubicacion[0] == ubicacion_proxima[0]+1 and ubicacion[1] == ubicacion_proxima[1]):\n",
    "                if(y==goal):\n",
    "                    R[casilla,y] = 100\n",
    "                else:\n",
    "                    R[casilla,y] = 0\n",
    "\n",
    "            if(ubicacion[0] == ubicacion_proxima[0] and ubicacion[1] == ubicacion_proxima[1]+1):\n",
    "                if(y==goal):\n",
    "                    R[casilla,y] = 100\n",
    "                else:\n",
    "                    R[casilla,y] = 0\n",
    "\n",
    "            if(ubicacion[0] == ubicacion_proxima[0]-1 and ubicacion[1] == ubicacion_proxima[1]):\n",
    "                if(y==goal):\n",
    "                    R[casilla,y] = 100\n",
    "                else:\n",
    "                    R[casilla,y] = 0\n",
    "\n",
    "            if(ubicacion[0] == ubicacion_proxima[0] and ubicacion[1] == ubicacion_proxima[1]-1):\n",
    "                if(y==goal):\n",
    "                    R[casilla,y] = 100\n",
    "                else:\n",
    "                    R[casilla,y] = 0\n",
    "\n",
    "\n",
    "            #COMPROBACIÓN DIAGONAL\n",
    "            if(ubicacion[0] == ubicacion_proxima[0]+1 and ubicacion[1] == ubicacion_proxima[1]+1):\n",
    "                if(y==goal):\n",
    "                    R[casilla,y] = 100\n",
    "                else:\n",
    "                    R[casilla,y] = 0\n",
    "\n",
    "            if(ubicacion[0] == ubicacion_proxima[0]-1 and ubicacion[1] == ubicacion_proxima[1]-1):\n",
    "                if(y==goal):\n",
    "                    R[casilla,y] = 100\n",
    "                else:\n",
    "                    R[casilla,y] = 0\n",
    "\n",
    "            if(ubicacion[0] == ubicacion_proxima[0]-1 and ubicacion[1] == ubicacion_proxima[1]+1):\n",
    "                if(y==goal):\n",
    "                    R[casilla,y] = 100\n",
    "                else:\n",
    "                    R[casilla,y] = 0\n",
    "\n",
    "            if(ubicacion[0] == ubicacion_proxima[0]+1 and ubicacion[1] == ubicacion_proxima[1]-1):\n",
    "                if(y==goal):\n",
    "                    R[casilla,y] = 100\n",
    "                else:\n",
    "                    R[casilla,y] = 0\n",
    "\n",
    "            #NO HAY ACTIONS HACIA UNO MISMO, EXCEPTO QUE SEAS LA META\n",
    "            if(casilla==y):\n",
    "                if(y==goal):\n",
    "                    R[casilla,y] = 100\n",
    "                else:\n",
    "                    R[casilla,y] = -1\n",
    "\n",
    "    \n",
    "    m2.celdas=[]\n",
    "    m2.valores=[]\n",
    "    text_q=Label(ventana, text=\"Matriz Q\")\n",
    "    text_q.grid(column=0,row=20)\n",
    "    \n",
    "    salida_Q=Label(ventana, text=Q)\n",
    "    salida_Q.grid(column=0,row=21)\n",
    "    m2.celdas.append(text_q)\n",
    "    m2.valores.append(salida_Q)\n",
    "\n",
    "    text_r=Label(ventana, text=\"Matriz R\")\n",
    "    text_r.grid(column=1, row=20)\n",
    "    salida_R = Label(ventana,text=R)\n",
    "    salida_R.grid(column=1, row=21)\n",
    "    m2.celdas.append(text_r)\n",
    "    m2.celdas.append(salida_R)\n",
    "\n",
    "    print(\"Matriz:\\n\", tablero)\n",
    "    print(\"Número de casillas: \", casillas)\n",
    "    print(\"\\nR (matriz de movimientos):\\n\", R)\n",
    "    print(\"\\nQ (matriz de aprendizaje):\\n\", Q)\n",
    "    print(\"\\n\")\n",
    "    start_program = Button(ventana, text=\"Entrenar matriz\", command=lambda: entrenamiento(inic_def, n_filas, start, Q, R,m2))\n",
    "    start_program.grid(column=0, row=n_filas+11)\n",
    "    start_program_alternative = Button(ventana, text=\"Entrenamiento alternativo\", command=lambda: entrenamiento_alternativo(inic_def, n_filas, start, Q, R,m2))\n",
    "    start_program_alternative.grid(column=1, row=n_filas+11)\n",
    "    start.append(start_program_alternative) #LLAMADA A ENTRENAR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenamiento(inic_def, n_filas, start, Q, R, m2):\n",
    "    \n",
    "    for c in m2.valores:\n",
    "        c.destroy()\n",
    "\n",
    "  \n",
    "    goal= int(entry_fin.get()) #META DEL RECORRIDO\n",
    "    episodios= int(entry_iteraciones.get())#NÚMERO DE ITERACIONES QUE TENDRÁ EL ENTRENAMIENTO\n",
    "    print(episodios)\n",
    "    gamma= float(entry_gamma.get()) #FACTOR DE APRENDIZAJE\n",
    "    state = random.randint(0,8) #ESTADO ALEATORIO DE INICIO DEL ENTRENAMIENTO \n",
    "    print(\"Estado inicial escogido: \", state)\n",
    "    print(\"Meta escogida: \", goal)\n",
    "    print(\"Gamma: \", gamma)\n",
    "    print(\"Episodios: \", episodios)\n",
    "\n",
    "    for x in range(episodios):\n",
    "        print(\"Estado actual: \", state)\n",
    "        actions = np.where(R[state]!=-1)[0] #LISTA DE ACCIONES QUE SON POSIBLES\n",
    "        print(\"Posibles acciones: \", actions)\n",
    "        random_action = random.choice(actions)\n",
    "        print(\"Acción escogida al azar: \", random_action) #ESCOJO UNA ACCIÓN ALEATORIAMENTE, LA CUAL SERÁ MI PRÓXIMO ESTADO\n",
    "        print(\"Peso en R de esa acción: \", R[state, random_action])\n",
    "        next_actions = np.where(R[random_action]!=-1)[0] #LISTA DE ACCIONES QUE SON POSIBLES EN EL PRÓXIMO ESTADO\n",
    "        print(\"Acciones que serán posibles cuando esté en\", random_action, \": \", next_actions)\n",
    "\n",
    "        next_actions_Qvalue = np.ones(len(next_actions), dtype=int)\n",
    "\n",
    "        for y in range(len(next_actions)):\n",
    "            next_actions_Qvalue[y] = Q[random_action, next_actions[y]]\n",
    "\n",
    "        print(\"Peso en Q para esas futuras acciones: \", next_actions_Qvalue)\n",
    "\n",
    "        Q[state, random_action] = R[state, random_action] + gamma * max(next_actions_Qvalue)\n",
    "        state=random_action\n",
    "        print(\"El rendimiento de este episodio de entrenamiento ha sido: \", (Q.sum()/np.max(Q))*100)\n",
    "        print(\"\\n\")\n",
    "        if(state==goal):\n",
    "            print(\"Hemos alcanzado la meta antes de terminar las iteraciones.\")\n",
    "            break\n",
    "    \n",
    "    \n",
    "    salida_Q=Label(ventana, text=Q)\n",
    "    salida_Q.grid(column=0,row=21)\n",
    "    m2.valores.append(salida_Q)\n",
    "    print(Q)\n",
    "    \n",
    "    finish_program = Button(ventana, text=\"Recorrer tablero\", command=lambda: recorrer_tablero(goal, Q, R))\n",
    "    finish_program.grid(column=0, row=n_filas+21)\n",
    "    start.append(finish_program) #LLAMADA A RECORRER EL TABLERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recorrer_tablero(goal_def, Q, R): \n",
    "\n",
    "    for c in recorrido.label:\n",
    "        c.destroy()\n",
    "        \n",
    "    state = int(entry_inic.get()) #ESTADO DE INICIO DEL RECORRIDO\n",
    "    goal = int(goal_def) #META DEL RECORRIDO\n",
    "    contador = 0 #NÚMERO DE MOVIMIENTOS USADOS PARA LLEGAR A LA META\n",
    "\n",
    "    print(\"El estado incial es: \", state)\n",
    "    print(\"La meta es: \", goal)\n",
    "\n",
    "    #VAMOS A HACER QUE EL AGENTE RECORRA EL TABLERO Y RECORRA EL MEJOR CAMINO\n",
    "    while state!=goal:\n",
    "        print(\"\\nEstado actual: \", state)\n",
    "        actions = np.where(R[state]!=-1)[0] #LISTA DE ACCIONES QUE SON POSIBLES EN ESTE MOMENTO\n",
    "        print(\"Posibles acciones: \", actions)\n",
    "\n",
    "        values = np.ones(len(actions), dtype=int) #PESO QUE TIENEN ESAS ACCIONES EN LA MATRIZ Q            \n",
    "        for y in range(len(actions)):\n",
    "            values[y] = Q[state, actions[y]]   \n",
    "        print(\"Pesos de esas acciones: \", values) \n",
    "\n",
    "        posicion_mayor_peso = np.where(values==np.max(values))[0][0] #LA POSICIÓN EN EL ARRAY DONDE ESTÁ LA ACCIÓN DE MAYOR PESO\n",
    "        print(\"Posición del mejor movimiento, en base a su peso: \", posicion_mayor_peso)\n",
    "        best_action = actions[posicion_mayor_peso] #LA MEJOR ACCIÓN POSIBLE\n",
    "        print(\"Mejor acción: \", best_action)\n",
    "\n",
    "        state=best_action\n",
    "        contador = contador + 1\n",
    "        \n",
    "        if(contador==20):\n",
    "            print(\"El agente no está lo suficientemente entrenado y no encuentra la meta, finalizando recorrido.\")\n",
    "            mb.showerror(\"Error\",\"El agente no está lo suficientemente entrenado y no encuentra la meta, finalizando recorrido.\")\n",
    "            break\n",
    "\n",
    "    if(contador<20):\n",
    "        print(\"Hicieron falta\", contador, \"movimientos para llegar a la meta.\\n\")\n",
    "        res = Label(ventana,text=\"Hicieron falta \" + str(contador) + \" movimientos para llegar a la meta.\")\n",
    "        res.grid(column=0,row=60)\n",
    "        recorrido.label.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]]\nEstado inicial escogido:  2\nMeta escogida:  6\nGamma:  0.5\nEpsilon:  7.864320000000009e-14\nAlpha:  0.2\nEpisodios:  3\nEstado actual:  2\nPosibles acciones:  [0 3 4 6 7]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  0\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 0 :  [2 3 6]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  0\nPosibles acciones:  [2 3 6]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  2\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 2 :  [0 3 4 6 7]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  2\nPosibles acciones:  [0 3 4 6 7]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  0\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 0 :  [2 3 6]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\n[[0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]]\nEstado inicial escogido:  0\nMeta escogida:  6\nGamma:  0.5\nEpsilon:  6.291456000000008e-16\nAlpha:  0.2\nEpisodios:  3\nEstado actual:  0\nPosibles acciones:  [2 3 6]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  2\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 2 :  [0 3 4 6 7]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  2\nPosibles acciones:  [0 3 4 6 7]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  0\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 0 :  [2 3 6]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  0\nPosibles acciones:  [2 3 6]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  2\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 2 :  [0 3 4 6 7]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\n[[0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]]\nEstado inicial escogido:  5\nMeta escogida:  6\nGamma:  0.5\nEpsilon:5.0331648000000065e-18\nAlpha:  0.2\nEpisodios:  3\nEstado actual:  5\nPosibles acciones:  [3 6 8]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  3\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 3 :  [0 2 5 6 8]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  3\nPosibles acciones:  [0 2 5 6 8]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  0\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 0 :  [2 3 6]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  0\nPosibles acciones:  [2 3 6]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  2\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 2 :  [0 3 4 6 7]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\n[[0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]]\nEstado inicial escogido:  2\nMeta escogida:  6\nGamma:  0.5\nEpsilon:  4.026531840000006e-20\nAlpha:  0.2\nEpisodios:  3\nEstado actual:  2\nPosibles acciones:  [0 3 4 6 7]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  0\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 0 :  [2 3 6]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  0\nPosibles acciones:  [2 3 6]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  2\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 2 :  [0 3 4 6 7]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  2\nPosibles acciones:  [0 3 4 6 7]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  0\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 0 :  [2 3 6]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\n[[0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]]\nEstado inicial escogido:  7\nMeta escogida:  6\nGamma:  0.5\nEpsilon:  3.2212254720000054e-22\nAlpha:  0.2\nEpisodios:  3\nEstado actual:  7\nPosibles acciones:  [2 4 6]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  2\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 2 :  [0 3 4 6 7]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  2\nPosibles acciones:  [0 3 4 6 7]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  0\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 0 :  [2 3 6]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  0\nPosibles acciones:  [2 3 6]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  2\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 2 :  [0 3 4 6 7]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\n[[0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]]\nEstado inicial escogido:  3\nMeta escogida:  6\nGamma:  0.5\nEpsilon:  2.576980377600005e-24\nAlpha:  0.2\nEpisodios:  3\nEstado actual:  3\nPosibles acciones:  [0 2 5 6 8]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  0\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 0 :  [2 3 6]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  0\nPosibles acciones:  [2 3 6]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  2\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 2 :  [0 3 4 6 7]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  2\nPosibles acciones:  [0 3 4 6 7]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  0\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 0 :  [2 3 6]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\n[[0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]]\nEstado inicial escogido:  7\nMeta escogida:  6\nGamma:  0.5\nEpsilon:  2.0615843020800046e-26\nAlpha:  0.2\nEpisodios:  3\nEstado actual:  7\nPosibles acciones:  [2 4 6]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  2\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 2 :  [0 3 4 6 7]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  2\nPosibles acciones:  [0 3 4 6 7]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  0\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 0 :  [2 3 6]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  0\nPosibles acciones:  [2 3 6]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  2\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 2 :  [0 3 4 6 7]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\n[[0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]]\nEstado inicial escogido:  6\nMeta escogida:  6\nGamma:  0.5\nEpsilon:  1.649267441664004e-28\nAlpha:  0.2\nEpisodios:  3\nEstado actual:  6\nPosibles acciones:  [0 1 2 3 4 5 6 7 8]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  0\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 0 :  [2 3 6]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  0\nPosibles acciones:  [2 3 6]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  2\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 2 :  [0 3 4 6 7]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  2\nPosibles acciones:  [0 3 4 6 7]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  0\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 0 :  [2 3 6]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\n[[0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]]\nEstado inicial escogido:  8\nMeta escogida:  6\nGamma:  0.5\nEpsilon:  1.3194139533312035e-30\nAlpha:  0.2\nEpisodios:  3\nEstado actual:  8\nPosibles acciones:  [1 3 4 5 6]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  1\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 1 :  [4 6 8]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  1\nPosibles acciones:  [4 6 8]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  4\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 4 :  [1 2 6 7 8]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  4\nPosibles acciones:  [1 2 6 7 8]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  1\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 1 :  [4 6 8]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\n[[0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]]\nEstado inicial escogido:  3\nMeta escogida:  6\nGamma:  0.5\nEpsilon:  1.055531162664963e-32\nAlpha:  0.2\nEpisodios:  3\nEstado actual:  3\nPosibles acciones:  [0 2 5 6 8]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  0\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 0 :  [2 3 6]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  0\nPosibles acciones:  [2 3 6]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  2\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 2 :  [0 3 4 6 7]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  2\nPosibles acciones:  [0 3 4 6 7]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  0\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 0 :  [2 3 6]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\n[[0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]]\nEstado inicial escogido:  3\nMeta escogida:  6\nGamma:  0.5\nEpsilon:  8.444249301319705e-35\nAlpha:  0.2\nEpisodios:  3\nEstado actual:  3\nPosibles acciones:  [0 2 5 6 8]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  0\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 0 :  [2 3 6]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  0\nPosibles acciones:  [2 3 6]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  2\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 2 :  [0 3 4 6 7]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  2\nPosibles acciones:  [0 3 4 6 7]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  0\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 0 :  [2 3 6]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\n[[0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]]\nEstado inicial escogido:  4\nMeta escogida:  6\nGamma:  0.5\nEpsilon:  6.755399441055764e-37\nAlpha:  0.2\nEpisodios:  3\nEstado actual:  4\nPosibles acciones:  [1 2 6 7 8]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  1\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 1 :  [4 6 8]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  1\nPosibles acciones:  [4 6 8]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  4\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 4 :  [1 2 6 7 8]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  4\nPosibles acciones:  [1 2 6 7 8]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  1\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 1 :  [4 6 8]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\n[[0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]]\nEstado inicial escogido:  3\nMeta escogida:  6\nGamma:  0.5\nEpsilon:  5.4043195528446125e-39\nAlpha:  0.2\nEpisodios:  3\nEstado actual:  3\nPosibles acciones:  [0 2 5 6 8]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  0\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 0 :  [2 3 6]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  0\nPosibles acciones:  [2 3 6]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  2\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 2 :  [0 3 4 6 7]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  2\nPosibles acciones:  [0 3 4 6 7]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  0\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 0 :  [2 3 6]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\n[[0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]]\nEstado inicial escogido:  1\nMeta escogida:  6\nGamma:  0.5\nEpsilon:  4.323455642275691e-41\nAlpha:  0.2\nEpisodios:  3\nEstado actual:  1\nPosibles acciones:  [4 6 8]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  4\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 4 :  [1 2 6 7 8]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  4\nPosibles acciones:  [1 2 6 7 8]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  1\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 1 :  [4 6 8]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  1\nPosibles acciones:  [4 6 8]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  4\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 4 :  [1 2 6 7 8]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\n[[0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]]\nEstado inicial escogido:  6\nMeta escogida:  6\nGamma:  0.5\nEpsilon:  3.4587645138205534e-43\nAlpha:  0.2\nEpisodios:  3\nEstado actual:  6\nPosibles acciones:  [0 1 2 3 4 5 6 7 8]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  0\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 0 :  [2 3 6]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  0\nPosibles acciones:  [2 3 6]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  2\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 2 :  [0 3 4 6 7]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  2\nPosibles acciones:  [0 3 4 6 7]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  0\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 0 :  [2 3 6]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\n[[0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]]\nEstado inicial escogido:  8\nMeta escogida:  6\nGamma:  0.5\nEpsilon:  2.767011611056443e-45\nAlpha:  0.2\nEpisodios:  3\nEstado actual:  8\nPosibles acciones:  [1 3 4 5 6]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  1\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 1 :  [4 6 8]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  1\nPosibles acciones:  [4 6 8]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  4\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 4 :  [1 2 6 7 8]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\nEstado actual:  4\nPosibles acciones:  [1 2 6 7 8]\nSe ha ejectuado la Fase 2.\nAcción escogida con el máximo Q:  1\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 1 :  [4 6 8]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  nan\n\n\n[[0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0]]\n"
    }
   ],
   "source": [
    "from tkinter import *\n",
    "from tkinter import messagebox as mb \n",
    "from random import sample   \n",
    "import numpy as np\n",
    "#Iniciamos la ventana Principal\n",
    "ventana = Tk()\n",
    "ventana.title(\"Aprendizaje por Refuerzo\")\n",
    "ventana.geometry('800x800')\n",
    "#ventana.configure(background = 'dark turquoise')\n",
    "\n",
    "#Añadimos las entradas de texto de Nº de Filas y Columnas\n",
    "filas = Label(ventana,text=\"Numero de Filas\")\n",
    "entry_fila = Entry(ventana, width=5)\n",
    "filas.grid(column=0,row=0)\n",
    "entry_fila.grid(column=1,row=0)\n",
    "\n",
    "columnas = Label(ventana,text=\"Numero de Columnas\")\n",
    "entry_columna = Entry(ventana, width=5)\n",
    "columnas.grid(column=0,row=3)\n",
    "entry_columna.grid(column=1,row=3)\n",
    "\n",
    "inic = Label(ventana,text=\"Posicion de inicio\")\n",
    "entry_inic = Entry(ventana, width=5)\n",
    "entry_inic.insert(END,0)\n",
    "inic.grid(column=2,row=0)\n",
    "entry_inic.grid(column=3,row=0)\n",
    "\n",
    "fin = Label(ventana,text=\"Objetivo\")\n",
    "entry_fin = Entry(ventana, width=5)\n",
    "entry_fin.insert(END,6)\n",
    "fin.grid(column=2,row=3)\n",
    "entry_fin.grid(column=3,row=3)\n",
    "\n",
    "gamma = Label(ventana,text=\"Valor Gamma\")\n",
    "entry_gamma = Entry(ventana, width=5)\n",
    "entry_gamma.insert(END,0.5)\n",
    "gamma.grid(column=4,row=0)\n",
    "entry_gamma.grid(column=5,row=0)\n",
    "\n",
    "iteraciones = Label(ventana,text=\"Numero de Iteraciones\")\n",
    "entry_iteraciones = Entry(ventana, width=5)\n",
    "entry_iteraciones.insert(END,3)\n",
    "iteraciones.grid(column=4,row=3)\n",
    "entry_iteraciones.grid(column=5,row=3)\n",
    "\n",
    "epsilon = Label(ventana,text=\"Valor Epsilon\")\n",
    "entry_epsilon = Entry(ventana, width=5)\n",
    "entry_epsilon.insert(END,0.3)\n",
    "epsilon.grid(column=6,row=0)\n",
    "entry_epsilon.grid(column=7,row=0)\n",
    "\n",
    "alpha = Label(ventana,text=\"Valor Alpha\")\n",
    "entry_alpha = Entry(ventana, width=5)\n",
    "entry_alpha.insert(END,0.2)\n",
    "alpha.grid(column=6,row=3)\n",
    "entry_alpha.grid(column=7,row=3)\n",
    "\n",
    "\n",
    "#Clase Matriz\n",
    "class Matrix():\n",
    "    def __init__(self,celdas,pos,valores,start,nueva_lista):\n",
    "        self.celdas=celdas[:]\n",
    "        self.pos=pos[:]\n",
    "        self.valores=valores[:]\n",
    "        self.start=start[:]\n",
    "        self.nueva_lista=nueva_lista[:]\n",
    "\n",
    "#Clase Recorrido (muestra datos de recorrido final)\n",
    "class Recorrido():\n",
    "    def __init__(self,label):\n",
    "        self.label=label[:]\n",
    "        \n",
    "recorrido = Recorrido([])\n",
    "\n",
    "#Clase EntrenamientoAlternativo\n",
    "class Alternative():\n",
    "    def __init__(self,label,epsilon,alfa):\n",
    "        self.label=label[:]\n",
    "        self.epsilon=epsilon[:]\n",
    "        self.alfa=alfa[:]\n",
    "\n",
    "alternative = Alternative([],[],[])\n",
    "\n",
    "#m1 instanciacion de matriz vacía\n",
    "m1 = Matrix([],[],[],[],[])\n",
    "m2 = Matrix([],[],[],[],[])\n",
    "\n",
    "\n",
    "def aleatorizar(e,n,valores):\n",
    "    \n",
    "    random = sample([x for x in range(0,n)],n)[0]\n",
    "    while(random in valores):\n",
    "        random = sample([x for x in range(0,n)],n)[0]\n",
    "    valores.append(random)\n",
    "    e.insert(END,random)\n",
    "\n",
    "\n",
    "def generarMatriz(m1):\n",
    "\n",
    "#En el caso de que m1 sea una matriz ya iniciada, esta tendrá celdas, valores, y posiciones que hay que eliminar previamente    \n",
    "    celdas = m1.celdas\n",
    "    pos = m1.pos\n",
    "    valores = m1.valores\n",
    "    nueva_lista=m1.nueva_lista\n",
    "    start=m1.start\n",
    "\n",
    "    del m1.pos[:]\n",
    "    del m1.valores[:]\n",
    "    for b in start:\n",
    "        b.destroy()\n",
    "    del m1.nueva_lista[:]\n",
    "    for c in celdas:\n",
    "        c.destroy()\n",
    "\n",
    "#Una vez eliminados validamos las entradas de nfilas y ncolumnas\n",
    "    validate_tam(entry_fila.get(),entry_columna.get())\n",
    "    n_filas=int(entry_fila.get())\n",
    "    n_columnas= int(entry_columna.get())\n",
    "    total = n_filas * n_columnas\n",
    "\n",
    "    # Creamos la tabla de entradas\n",
    "    for row in range(n_filas):\n",
    "        for column in range(n_columnas):\n",
    "            index = (row, column)\n",
    "            e = Entry(ventana, width=10)\n",
    "            aleatorizar(e,total,valores)\n",
    "            e.grid(row=row+10, column=column, stick=\"nsew\")\n",
    "            celdas.append(e)\n",
    "            pos.append(index)\n",
    "\n",
    "            \n",
    "    m1.celdas= celdas\n",
    "    m1.pos = pos\n",
    "    m1.valores= valores\n",
    "    result = zip(pos,valores)\n",
    "    result_set = set(result)\n",
    "    inic_QR = Button(ventana, text= \"Generar Q y R\", command=lambda: \n",
    "    iniciar_tablero(n_filas, start,m2))\n",
    "    inic_QR.grid(column=2, row=4)\n",
    "    #AQUÍ SE HACE LA MAGIA\n",
    "\n",
    "def validate_tam(n_filas,n_columnas):\n",
    "    try:\n",
    "        f= int(n_filas)\n",
    "        c= int(n_columnas)\n",
    "    except ValueError:\n",
    "        mb.showerror(\"Error\",\"Debe introducir un valor entero como numero de filas y columnas\")\n",
    "    \n",
    "    if(int(n_filas) == 0 or (n_columnas) == 0):\n",
    "        mb.showerror(\"Error\",\"Ni la fila ni la columna puede ser de tamaño 0\")\n",
    "    \n",
    "#Creamos la matriz según las entradas\n",
    "btn = Button(ventana, text=\"Generar Matriz\", command=lambda: generarMatriz(m1))\n",
    "btn.grid(column=0, row=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ventana.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "valores = [0,\"T3\",\"B5\",\"T6\",2,7,1,4,8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264biteb45e1f164454beda4749fe81198b568"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
