{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "def iniciar_tablero(n_filas, start,m2):\n",
    "\n",
    "    for c in m2.valores:\n",
    "        c.destroy()\n",
    "    for c in m2.celdas:\n",
    "        c.destroy()\n",
    "    del m1.nueva_lista[:]\n",
    "    \n",
    "    #LECTURA DE LA MATRIZ DE LA INTERFAZ DE USUARIO, ES LA QUE USARÉ\n",
    "    n_columnas= int(entry_columna.get())\n",
    "    lista_tablero=[]\n",
    "    T=[]    #Array de valores de la matriz que son TRAMPA\n",
    "    B=[]    #Array de valores de la matriz que son BONIFICACIONES\n",
    "    for c in m1.celdas:\n",
    "        try:\n",
    "            lista_tablero.append(int(c.get()))  #INTENTO PASAR A ENTERO EL VALOR DE LA CASILLA, SI NO ME DEJA ES PORQUE ES UNA CASILLA TRAMPA O BONUS\n",
    "        except ValueError:\n",
    "            recompensa = str(c.get())\n",
    "            if(recompensa.startswith(\"T\")):             #SI EMPIEZA POR T ES TRAMPA -> (T,6)\n",
    "                T_aux = recompensa.split(\",\")           #AUX ES LISTA DE DOS ELEMENTOS [[T],[6]]\n",
    "                T_aux.remove(\"T\")                       #ELIMINO LA T DE LA LISTA\n",
    "                val=T_aux[0]                            #ME QUEDO CON EL NUMERO\n",
    "                T.append(int(val))\n",
    "                lista_tablero.append(int(val))\n",
    "            else:\n",
    "                B_aux = recompensa.split(\",\")\n",
    "                B_aux.remove(\"B\")\n",
    "                val=B_aux[0]\n",
    "                B.append(int(val))            \n",
    "                lista_tablero.append(int(val))\n",
    "\n",
    "    set_tablero= set(lista_tablero)\n",
    "\n",
    "    inic_def= int(entry_inic.get())\n",
    "    goal_def= int(entry_fin.get())\n",
    "    iteraciones= int(entry_iteraciones.get())\n",
    "    gamma_def= float(entry_gamma.get()) \n",
    "\n",
    "    #VALIDACION DE FORMULARIO#\n",
    "    #VALIDACION INICIO Y GOAL DENTRO DE MATRIZ DE NUMEROS\n",
    "    if(not(goal_def in lista_tablero) or not(inic_def in lista_tablero)):\n",
    "        mb.showerror(\"Error\",\"Posición de Inicio y Objetivo han de ser números que existan en la matriz\")\n",
    "    #MATRIZ DE NUMEROS\n",
    "    if(len(set_tablero) < len(lista_tablero)):\n",
    "        mb.showerror(\"Error\",\"No puede repetir numero en la matriz\")\n",
    "\n",
    "    #VALIDACION DE TRAMPA Y BONUS: NO PUEDE HABER TRAMPA NI BONUS EN INICIO NI FIN\n",
    "    if((goal_def in T) or (goal_def in B) or (inic_def in T) or (inic_def in B)):\n",
    "        mb.showerror(\"Error\",\"Posición de Inicio y Objetivo no deben de ser posiciones de bonus ni trampa\")\n",
    "\n",
    "    #VALIDACION GAMMA EPSILON Y ALFA: ENTRE 0 Y 1\n",
    "    if(not(gamma_def>0 and gamma_def<1)):\n",
    "        mb.showerror(\"Error\",\"Gamma debe de estar entre 0 y 1\")\n",
    "\n",
    "    print(\"T\",T)\n",
    "    print(\"B\",B)\n",
    "    print(\"Lista\",lista_tablero)\n",
    "    print(\"Set\",set_tablero)\n",
    "    \n",
    "    lista_tablero = [lista_tablero[i:i+n_columnas] for i in range(0, len(lista_tablero), n_columnas)]\n",
    "    m1.nueva_lista=lista_tablero\n",
    "\n",
    "    #A CONTINUACIÓN VAMOS A DEFINIR TODAS LAS PROPIEDADES DEL PROBLEMA\n",
    "    tablero_array = [[0,1],[2,3],[4,5]]\n",
    "    #tablero_array = [[0,1,2],[3,4,5],[6,7,8]]\n",
    "    tablero_array = lista_tablero #TABLERO QUE VAMOS A UTILIZAR\n",
    "    tablero = np.array(tablero_array) #AQUÍ LO CONVERTIMOS EN UNA MATRIZ\n",
    "    casillas = len(tablero)*len(tablero[0]) #NÚMERO DE CASILLAS DEL TABLERO\n",
    "    goal = int(goal_def) #META PARA TERMINAR EL ENTRENAMIENTO\n",
    "    Q = np.zeros((casillas, casillas), dtype = int) #MATRIZ DE ENTRENAMIENTO INICIALIZADA A 0\n",
    "    R = np.full((casillas, casillas), -1) #MATRIZ DE MOVIMIENTOS INICIALIZADA A -1\n",
    "\n",
    "    #DECLARACIÓN DE R, LA MATRIZ DE MOVIMIENTOS POSIBLES. UNA POSICIÓN DE ESTA MATRIZ TENDRÁ EL VALOR 0 CUANDO EXISTA CAMINO\n",
    "    #ENTRE X (STATE) E Y (ACTION). SI UNO DE ESOS CAMINOS VÁLIDOS CONDUCE A LA META, EL VALOR PASARÁ A SER 100.\n",
    "    #PARA CAMINOS NO VÁLIDOS (POR NO SER CASILLAS VECINAS), EL VALOR SERÁ -1\n",
    "    #HEMOS CONSIDERADO QUE LA META ES LA ÚNICA CASILLA CON UN CAMINO QUE LA LLEVA DE VUELTA A ELLA MISMA\n",
    "\n",
    "    for casilla in range(casillas): #VAMOS A ITERAR POR TODAS LAS CASILLAS DEL TABLERO\n",
    "        ubicacion = np.where(tablero==casilla) #ALMACENAMOS LAS COORDENADAS DE LA CASILLA DE LA ITERACIÓN ACTUAL\n",
    "\n",
    "        for y in range(casillas):\n",
    "            ubicacion_proxima=np.where(tablero==y) #ALMACENAMOS LAS COORDENADAS DE OTRA CASILLA DEL TABLERO PARA COMPROBAR SU\n",
    "                                                   #CERCANÍA CON LA CASILLA DE LA ITERACIÓN ACTUAL\n",
    "            #COMPROBACIÓN HORIZONTAL Y VERTICAL\n",
    "            if(ubicacion[0] == ubicacion_proxima[0]+1 and ubicacion[1] == ubicacion_proxima[1]):\n",
    "                if(y==goal):\n",
    "                    R[casilla,y] = 100\n",
    "                elif(y in B):\n",
    "                    R[casilla,y] = 60\n",
    "                elif(y in T):\n",
    "                    R[casilla,y] = -1\n",
    "                else:\n",
    "                    R[casilla,y] = 0\n",
    "\n",
    "            if(ubicacion[0] == ubicacion_proxima[0] and ubicacion[1] == ubicacion_proxima[1]+1):\n",
    "                if(y==goal):\n",
    "                    R[casilla,y] = 100\n",
    "                elif(y in B):\n",
    "                    R[casilla,y] = 60\n",
    "                elif(y in T):\n",
    "                    R[casilla,y] = -1\n",
    "                else:\n",
    "                    R[casilla,y] = 0\n",
    "\n",
    "            if(ubicacion[0] == ubicacion_proxima[0]-1 and ubicacion[1] == ubicacion_proxima[1]):\n",
    "                if(y==goal):\n",
    "                    R[casilla,y] = 100\n",
    "                elif(y in B):\n",
    "                    R[casilla,y] = 60\n",
    "                elif(y in T):\n",
    "                    R[casilla,y] = -1\n",
    "                else:\n",
    "                    R[casilla,y] = 0\n",
    "\n",
    "            if(ubicacion[0] == ubicacion_proxima[0] and ubicacion[1] == ubicacion_proxima[1]-1):\n",
    "                if(y==goal):\n",
    "                    R[casilla,y] = 100\n",
    "                elif(y in B):\n",
    "                    R[casilla,y] = 60\n",
    "                elif(y in T):\n",
    "                    R[casilla,y] = -1\n",
    "                else:\n",
    "                    R[casilla,y] = 0\n",
    "\n",
    "\n",
    "            #COMPROBACIÓN DIAGONAL\n",
    "            if(ubicacion[0] == ubicacion_proxima[0]+1 and ubicacion[1] == ubicacion_proxima[1]+1):\n",
    "                if(y==goal):\n",
    "                    R[casilla,y] = 100\n",
    "                elif(y in B):\n",
    "                    R[casilla,y] = 60\n",
    "                elif(y in T):\n",
    "                    R[casilla,y] = -1\n",
    "                else:\n",
    "                    R[casilla,y] = 0\n",
    "\n",
    "            if(ubicacion[0] == ubicacion_proxima[0]-1 and ubicacion[1] == ubicacion_proxima[1]-1):\n",
    "                if(y==goal):\n",
    "                    R[casilla,y] = 100\n",
    "                elif(y in B):\n",
    "                    R[casilla,y] = 60\n",
    "                elif(y in T):\n",
    "                    R[casilla,y] = -1\n",
    "                else:\n",
    "                    R[casilla,y] = 0\n",
    "\n",
    "            if(ubicacion[0] == ubicacion_proxima[0]-1 and ubicacion[1] == ubicacion_proxima[1]+1):\n",
    "                if(y==goal):\n",
    "                    R[casilla,y] = 100\n",
    "                elif(y in B):\n",
    "                    R[casilla,y] = 60\n",
    "                elif(y in T):\n",
    "                    R[casilla,y] = -1\n",
    "                else:\n",
    "                    R[casilla,y] = 0\n",
    "\n",
    "            if(ubicacion[0] == ubicacion_proxima[0]+1 and ubicacion[1] == ubicacion_proxima[1]-1):\n",
    "                if(y==goal):\n",
    "                    R[casilla,y] = 100\n",
    "                elif(y in B):\n",
    "                    R[casilla,y] = 60\n",
    "                elif(y in T):\n",
    "                    R[casilla,y] = -1\n",
    "                else:\n",
    "                    R[casilla,y] = 0\n",
    "\n",
    "            #NO HAY ACTIONS HACIA UNO MISMO, EXCEPTO QUE SEAS LA META\n",
    "            if(casilla==y):\n",
    "                if(y==goal):\n",
    "                    R[casilla,y] = 100\n",
    "                else:\n",
    "                    R[casilla,y] = -1\n",
    "\n",
    "    \n",
    "    m2.celdas=[]\n",
    "    m2.valores=[]\n",
    "    text_q=Label(ventana, text=\"Matriz Q\")\n",
    "    text_q.grid(column=0,row=20)\n",
    "    \n",
    "    salida_Q=Label(ventana, text=Q)\n",
    "    salida_Q.grid(column=0,row=21)\n",
    "    m2.celdas.append(text_q)\n",
    "    m2.valores.append(salida_Q)\n",
    "\n",
    "    text_r=Label(ventana, text=\"Matriz R\")\n",
    "    text_r.grid(column=1, row=20)\n",
    "    salida_R = Label(ventana,text=R)\n",
    "    salida_R.grid(column=1, row=21)\n",
    "    m2.celdas.append(text_r)\n",
    "    m2.celdas.append(salida_R)\n",
    "\n",
    "    print(\"Matriz:\\n\", tablero)\n",
    "    print(\"Número de casillas: \", casillas)\n",
    "    print(\"\\nR (matriz de movimientos):\\n\", R)\n",
    "    print(\"\\nQ (matriz de aprendizaje):\\n\", Q)\n",
    "    print(\"\\n\")\n",
    "    start_program = Button(ventana, text=\"Entrenar matriz\", command=lambda: entrenamiento(inic_def, n_filas, start, Q, R,m2))\n",
    "    start_program.grid(column=0, row=n_filas+11)\n",
    "    start_program_alternative = Button(ventana, text=\"Entrenamiento alternativo\", command=lambda: entrenamiento_alternativo(inic_def, n_filas, start, Q, R,m2))\n",
    "    start_program_alternative.grid(column=1, row=n_filas+11)\n",
    "    start.append(start_program_alternative) #LLAMADA A ENTRENAR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "matplotlib.use(\"TkAgg\") \n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_tkagg import (FigureCanvasTkAgg,NavigationToolbar2Tk)\n",
    "from numpy import *\n",
    "import networkx as nx\n",
    "\n",
    "def entrenamiento(inic_def, n_filas, start, Q, R, m2):\n",
    "    \n",
    "    for c in m2.valores:\n",
    "        c.destroy()\n",
    "\n",
    "    goal= int(entry_fin.get()) #META DEL RECORRIDO\n",
    "    episodios= int(entry_iteraciones.get())#NÚMERO DE ITERACIONES QUE TENDRÁ EL ENTRENAMIENTO\n",
    "    gamma= float(entry_gamma.get()) #FACTOR DE APRENDIZAJE\n",
    "    state = random.randint(0,8) #ESTADO ALEATORIO DE INICIO DEL ENTRENAMIENTO \n",
    "    print(\"Estado inicial escogido: \", state)\n",
    "    print(\"Meta escogida: \", goal)\n",
    "    print(\"Gamma: \", gamma)\n",
    "    print(\"Episodios: \", episodios)\n",
    "    \n",
    "    if(len(datos.entrenamientos)==0):\n",
    "        cont= 1\n",
    "    else:\n",
    "        cont=datos.entrenamientos[-1]+1\n",
    "\n",
    "    edges=[]    #Aristas del grafo\n",
    "    edges_choice=[] #Aristas de las acciones escogidas\n",
    "\n",
    "    for x in range(episodios):\n",
    "        print(\"Estado actual: \", state)\n",
    "        actions = np.where(R[state]!=-1)[0] #LISTA DE ACCIONES QUE SON POSIBLES\n",
    "        print(\"Posibles acciones: \", actions)\n",
    "        random_action = random.choice(actions)\n",
    "        print(\"Acción escogida al azar: \", random_action) #ESCOJO UNA ACCIÓN ALEATORIAMENTE, LA CUAL SERÁ MI PRÓXIMO ESTADO\n",
    "        print(\"Peso en R de esa acción: \", R[state, random_action])\n",
    "        next_actions = np.where(R[random_action]!=-1)[0] #LISTA DE ACCIONES QUE SON POSIBLES EN EL PRÓXIMO ESTADO\n",
    "        print(\"Acciones que serán posibles cuando esté en\", random_action, \": \", next_actions)\n",
    "\n",
    "        for a in actions:\n",
    "            edges.append((state,a))\n",
    "\n",
    "        edges_choice.append((state,random_action))    #COLOR ROJO\n",
    "\n",
    "        next_actions_Qvalue = np.ones(len(next_actions), dtype=int)\n",
    "\n",
    "        for y in range(len(next_actions)):\n",
    "            next_actions_Qvalue[y] = Q[random_action, next_actions[y]]\n",
    "\n",
    "        print(\"Peso en Q para esas futuras acciones: \", next_actions_Qvalue)\n",
    "\n",
    "        Q[state, random_action] = R[state, random_action] + gamma * max(next_actions_Qvalue)\n",
    "        state=random_action\n",
    "        print(\"El rendimiento de este episodio de entrenamiento ha sido: \", (Q.sum()/np.max(Q))*100)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        datos.rendimientos = np.append(datos.rendimientos,(Q.sum()/np.max(Q))*100)\n",
    "        datos.entrenamientos= np.append(datos.entrenamientos,cont)\n",
    "        cont=cont+1\n",
    "\n",
    "        print(edges)\n",
    "        if(state==goal):\n",
    "            print(\"Hemos alcanzado la meta antes de terminar las iteraciones.\")\n",
    "            break\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(edges)\n",
    "    black_edges = [edge for edge in G.edges() if edge not in edges_choice]\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw_networkx_nodes(G, pos, cmap=plt.get_cmap('jet'), node_size = 500)\n",
    "    nx.draw_networkx_labels(G, pos)\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=edges_choice, edge_color='r', arrows=True)\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=black_edges, arrows=False)\n",
    "    plt.show()\n",
    "    #Cambiar los NaN obtenidos de rendimiento por 0\n",
    "    where_are_NaNs = isnan(datos.rendimientos)\n",
    "    datos.rendimientos[where_are_NaNs] = 0\n",
    "\n",
    "    print(\"Entrenamientos\",datos.entrenamientos)\n",
    "    print(\"Rendimientos\",datos.rendimientos)\n",
    "    graph = Tk()\n",
    "    graph.title(\"Rendimientos\")\n",
    "    graph.geometry('800x800')\n",
    "    f = Figure(figsize=(5,5),dpi=100)\n",
    "    a = f.add_subplot(111)\n",
    "    a.plot(datos.entrenamientos, datos.rendimientos)\n",
    "    a.set_ylabel('Rendimiento')\n",
    "    a.set_xlabel('Episodio de entrenamiento')\n",
    "\n",
    "    canvas = FigureCanvasTkAgg(f,master=graph)\n",
    "    canvas.draw()\n",
    "    canvas.get_tk_widget().pack(side=TOP, fill=BOTH, expand=1)\n",
    "\n",
    "    toolbar = NavigationToolbar2Tk(canvas, graph)\n",
    "    toolbar.update()\n",
    "    canvas.get_tk_widget().pack(side=TOP, fill=BOTH, expand=1)\n",
    "\n",
    "    salida_Q=Label(ventana, text=Q)\n",
    "    salida_Q.grid(column=0,row=21)\n",
    "    m2.valores.append(salida_Q)\n",
    "    print(Q)\n",
    "    \n",
    "    finish_program = Button(ventana, text=\"Recorrer tablero\", command=lambda: recorrer_tablero(goal, Q, R))\n",
    "    finish_program.grid(column=0, row=n_filas+21)\n",
    "    start.append(finish_program) #LLAMADA A RECORRER EL TABLERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenamiento_alternativo(inic_def, n_filas, start, Q, R, m2):\n",
    "    \n",
    "    for c in m2.valores:\n",
    "        c.destroy()\n",
    "\n",
    "    #inic_def= int(entry_inic.get())\n",
    "    goal= int(entry_fin.get()) #META DEL RECORRIDO\n",
    "    episodios= int(entry_iteraciones.get())#NÚMERO DE ITERACIONES QUE TENDRÁ EL ENTRENAMIENTO\n",
    "    gamma= float(entry_gamma.get()) #FACTOR DE APRENDIZAJE\n",
    "    state = random.randint(0,len(Q[0])-1) #ESTADO ALEATORIO DE INICIO DEL ENTRENAMIENTO \n",
    "    \n",
    "    epsilon = float(entry_epsilon.get()) #HAY QUE METERLO POR INTERFAZ DE USUARIO\n",
    "    alpha =  float(entry_alpha.get())  #HAY QUE METERLO POR INTERFAZ DE USUARIO\n",
    "    \n",
    "    #VALIDACION DE EPSILON Y ALPHA\n",
    "\n",
    "\n",
    "    print(\"Estado inicial escogido: \", state)\n",
    "    print(\"Meta escogida: \", goal)\n",
    "    print(\"Gamma: \", gamma)\n",
    "    print(\"Epsilon: \", epsilon)\n",
    "    print(\"Alpha: \", alpha)\n",
    "    print(\"Episodios: \", episodios)\n",
    "\n",
    "    if(len(datos.entrenamientos)==0):\n",
    "        cont= 1\n",
    "    else:\n",
    "        cont=datos.entrenamientos[-1]+1\n",
    "\n",
    "    edges=[]    #Aristas del grafo\n",
    "    edges_choice=[] #Aristas de las acciones escogidas\n",
    "\n",
    "    for x in range(episodios):\n",
    "        print(\"Estado actual: \", state)\n",
    "        actions = np.where(R[state]!=-1)[0] #LISTA DE ACCIONES QUE SON POSIBLES\n",
    "        print(\"Posibles acciones: \", actions)\n",
    "        \n",
    "        for a in actions:\n",
    "            edges.append((state,a))\n",
    "\n",
    "        if(random.uniform(0, 1) < epsilon):\n",
    "            print(\"Se ha ejecutado la Fase 1.\")\n",
    "            next_state = random.choice(actions) #ESCOJO UNA ACCIÓN ALEATORIAMENTE, LA CUAL SERÁ MI PRÓXIMO ESTADO\n",
    "            print(\"Acción escogida al azar: \", next_state)\n",
    "        else:\n",
    "            print(\"Se ha ejecutado la Fase 2.\")\n",
    "            next_state = 0\n",
    "            Q_aux = -1\n",
    "            \n",
    "            for st in actions:                  #EN ESTE BUCLE NOS QUEDAREMOS CON EL PRÓXIMO ESTADO CON MAYOR VALOR EN Q\n",
    "                if(Q[state, st] > Q_aux):            \n",
    "                    Q_aux = Q[state, st]\n",
    "                    next_state = st\n",
    "\n",
    "            print(\"Acción escogida con el máximo Q: \", next_state)\n",
    "\n",
    "        edges_choice.append((state,next_state))    #COLOR ROJO DEL GRAFO, ACCIONES TOMADAS\n",
    "\n",
    "        print(\"Peso en R de esa acción: \", R[state, next_state])\n",
    "        next_actions = np.where(R[next_state]!=-1)[0] #LISTA DE ACCIONES QUE SON POSIBLES EN EL PRÓXIMO ESTADO\n",
    "        print(\"Acciones que serán posibles cuando esté en\", next_state, \": \", next_actions)\n",
    "\n",
    "        next_actions_Qvalue = np.ones(len(next_actions), dtype=int)\n",
    "\n",
    "        for y in range(len(next_actions)):            #MEDIANTE ESTE BUCLE, DEJAREMOS PREPARADO UN ARRAY CON LOS VALORES EN Q \n",
    "            next_actions_Qvalue[y] = Q[next_state, next_actions[y]]        #DEL FUTURO ESTADO, ASÍ PODREMOS SACARLE EL MAX\n",
    "\n",
    "        print(\"Peso en Q para esas futuras acciones: \", next_actions_Qvalue)\n",
    "\n",
    "        Q[state, next_state] = R[state, next_state] + gamma * max(next_actions_Qvalue)\n",
    "        state = next_state\n",
    "        epsilon = epsilon * alpha\n",
    "        \n",
    "        print(\"El rendimiento de este episodio de entrenamiento ha sido: \", (Q.sum()/np.max(Q))*100) #RENDIMIENTO\n",
    "        print(\"\\n\")\n",
    "\n",
    "        datos.rendimientos = np.append(datos.rendimientos,(Q.sum()/np.max(Q))*100)\n",
    "        datos.entrenamientos= np.append(datos.entrenamientos,cont)\n",
    "        cont=cont+1\n",
    "\n",
    "        if(state==goal):\n",
    "            print(\"Hemos alcanzado la meta antes de terminar las iteraciones.\")\n",
    "            break\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(edges)\n",
    "    black_edges = [edge for edge in G.edges() if edge not in edges_choice]\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw_networkx_nodes(G, pos, cmap=plt.get_cmap('jet'), node_size = 500)\n",
    "    nx.draw_networkx_labels(G, pos)\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=edges_choice, edge_color='r', arrows=True)\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=black_edges, arrows=False)\n",
    "    plt.show()\n",
    "\n",
    "    #Cambiar los NaN obtenidos de rendimiento por 0\n",
    "    where_are_NaNs = isnan(datos.rendimientos)\n",
    "    datos.rendimientos[where_are_NaNs] = 0\n",
    "\n",
    "    print(\"Entrenamientos\",datos.entrenamientos)\n",
    "    print(\"Rendimientos etrenamiento\",datos.rendimientos)\n",
    "\n",
    "    graph = Tk()\n",
    "    graph.title(\"Rendimientos Alternativos\")\n",
    "    graph.geometry('800x800')\n",
    "    f = Figure(figsize=(5,5),dpi=100)\n",
    "    a = f.add_subplot(111)\n",
    "    a.plot(datos.entrenamientos, datos.rendimientos)\n",
    "    a.set_ylabel('Rendimiento')\n",
    "    a.set_xlabel('Episodio de entrenamiento')\n",
    "\n",
    "    canvas = FigureCanvasTkAgg(f,master=graph)\n",
    "    canvas.draw()\n",
    "    canvas.get_tk_widget().pack(side=TOP, fill=BOTH, expand=1)\n",
    "\n",
    "    toolbar = NavigationToolbar2Tk(canvas, graph)\n",
    "    toolbar.update()\n",
    "    canvas.get_tk_widget().pack(side=TOP, fill=BOTH, expand=1)\n",
    "    \n",
    "    entry_epsilon.delete(0,END)\n",
    "    entry_epsilon.insert(END,epsilon) #ACTUALIZAMOS EL VALOR DE EPSILON EN LA INTERFAZ DE USUARIO\n",
    "    \n",
    "    salida_Q=Label(ventana, text=Q)\n",
    "    salida_Q.grid(column=0,row=21)\n",
    "    m2.valores.append(salida_Q)\n",
    "    print(Q)\n",
    "    \n",
    "    finish_program = Button(ventana, text=\"Recorrer tablero\", command=lambda: recorrer_tablero(goal, Q, R))\n",
    "    finish_program.grid(column=0, row=n_filas+21)\n",
    "    start.append(finish_program) #LLAMADA A RECORRER EL TABLERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recorrer_tablero(goal_def, Q, R): \n",
    "\n",
    "    for c in recorrido.label:\n",
    "        c.destroy()\n",
    "        \n",
    "    state = int(entry_inic.get()) #ESTADO DE INICIO DEL RECORRIDO\n",
    "    goal = int(goal_def) #META DEL RECORRIDO\n",
    "    contador = 0 #NÚMERO DE MOVIMIENTOS USADOS PARA LLEGAR A LA META\n",
    "\n",
    "    print(\"El estado incial es: \", state)\n",
    "    print(\"La meta es: \", goal)\n",
    "\n",
    "    edges=[]    #Aristas del grafo\n",
    "    edges_choice=[] #Aristas de las acciones escogidas\n",
    "    \n",
    "    #VAMOS A HACER QUE EL AGENTE RECORRA EL TABLERO Y RECORRA EL MEJOR CAMINO\n",
    "    while state!=goal:\n",
    "        print(\"\\nEstado actual: \", state)\n",
    "        actions = np.where(R[state]!=-1)[0] #LISTA DE ACCIONES QUE SON POSIBLES EN ESTE MOMENTO\n",
    "        print(\"Posibles acciones: \", actions)\n",
    "\n",
    "        for a in actions:                   #AÑADIR VERTICES DE POSIBLES ACCIONES\n",
    "            edges.append((state,a))\n",
    "\n",
    "        values = np.ones(len(actions), dtype=int) #PESO QUE TIENEN ESAS ACCIONES EN LA MATRIZ Q            \n",
    "        for y in range(len(actions)):\n",
    "            values[y] = Q[state, actions[y]]   \n",
    "        print(\"Pesos de esas acciones: \", values) \n",
    "\n",
    "        posicion_mayor_peso = np.where(values==np.max(values))[0][0] #LA POSICIÓN EN EL ARRAY DONDE ESTÁ LA ACCIÓN DE MAYOR PESO\n",
    "        print(\"Posición del mejor movimiento, en base a su peso: \", posicion_mayor_peso)\n",
    "        best_action = actions[posicion_mayor_peso] #LA MEJOR ACCIÓN POSIBLE\n",
    "        print(\"Mejor acción: \", best_action)\n",
    "\n",
    "        edges_choice.append((state,best_action))    #COLOR ROJO DEL GRAFO, ACCIONES TOMADAS\n",
    "\n",
    "        state=best_action\n",
    "        contador = contador + 1\n",
    "        \n",
    "        if(contador==20):\n",
    "            print(\"El agente no está lo suficientemente entrenado y no encuentra la meta, finalizando recorrido.\")\n",
    "            mb.showerror(\"Error\",\"El agente no está lo suficientemente entrenado y no encuentra la meta, finalizando recorrido.\")\n",
    "            break\n",
    "\n",
    "    if(contador<20):\n",
    "        print(\"Hicieron falta\", contador, \"movimientos para llegar a la meta.\\n\")\n",
    "        res = Label(ventana,text=\"Hicieron falta \" + str(contador) + \" movimientos para llegar a la meta.\")\n",
    "        res.grid(column=0,row=60)\n",
    "        recorrido.label.append(res)\n",
    "\n",
    "        G = nx.DiGraph()\n",
    "        G.add_edges_from(edges)\n",
    "        black_edges = [edge for edge in G.edges() if edge not in edges_choice]\n",
    "        pos = nx.spring_layout(G)\n",
    "        nx.draw_networkx_nodes(G, pos, cmap=plt.get_cmap('jet'), node_size = 500)\n",
    "        nx.draw_networkx_labels(G, pos)\n",
    "        nx.draw_networkx_edges(G, pos, edgelist=edges_choice, edge_color='r', arrows=True)\n",
    "        nx.draw_networkx_edges(G, pos, edgelist=black_edges, arrows=False)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sibles acciones:  [0 2 8]\nPesos de esas acciones:  [0 0 0]\nPosición del mejor movimiento, en base a su peso:  0\nMejor acción:  0\n\nEstado actual:  0\nPosibles acciones:  [1 2 5 7 8]\nPesos de esas acciones:  [0 0 0 0 0]\nPosición del mejor movimiento, en base a su peso:  0\nMejor acción:  1\n\nEstado actual:  1\nPosibles acciones:  [0 2 8]\nPesos de esas acciones:  [0 0 0]\nPosición del mejor movimiento, en base a su peso:  0\nMejor acción:  0\n\nEstado actual:  0\nPosibles acciones:  [1 2 5 7 8]\nPesos de esas acciones:  [0 0 0 0 0]\nPosición del mejor movimiento, en base a su peso:  0\nMejor acción:  1\n\nEstado actual:  1\nPosibles acciones:  [0 2 8]\nPesos de esas acciones:  [0 0 0]\nPosición del mejor movimiento, en base a su peso:  0\nMejor acción:  0\n\nEstado actual:  0\nPosibles acciones:  [1 2 5 7 8]\nPesos de esas acciones:  [0 0 0 0 0]\nPosición del mejor movimiento, en base a su peso:  0\nMejor acción:  1\n\nEstado actual:  1\nPosibles acciones:  [0 2 8]\nPesos de esas acciones:  [0 0 0]\nPosición del mejor movimiento, en base a su peso:  0\nMejor acción:  0\n\nEstado actual:  0\nPosibles acciones:  [1 2 5 7 8]\nPesos de esas acciones:  [0 0 0 0 0]\nPosición del mejor movimiento, en base a su peso:  0\nMejor acción:  1\n\nEstado actual:  1\nPosibles acciones:  [0 2 8]\nPesos de esas acciones:  [0 0 0]\nPosición del mejor movimiento, en base a su peso:  0\nMejor acción:  0\n\nEstado actual:  0\nPosibles acciones:  [1 2 5 7 8]\nPesos de esas acciones:  [0 0 0 0 0]\nPosición del mejor movimiento, en base a su peso:  0\nMejor acción:  1\n\nEstado actual:  1\nPosibles acciones:  [0 2 8]\nPesos de esas acciones:  [0 0 0]\nPosición del mejor movimiento, en base a su peso:  0\nMejor acción:  0\n\nEstado actual:  0\nPosibles acciones:  [1 2 5 7 8]\nPesos de esas acciones:  [0 0 0 0 0]\nPosición del mejor movimiento, en base a su peso:  0\nMejor acción:  1\n\nEstado actual:  1\nPosibles acciones:  [0 2 8]\nPesos de esas acciones:  [0 0 0]\nPosición del mejor movimiento, en base a su peso:  0\nMejor acción:  0\n\nEstado actual:  0\nPosibles acciones:  [1 2 5 7 8]\nPesos de esas acciones:  [0 0 0 0 0]\nPosición del mejor movimiento, en base a su peso:  0\nMejor acción:  1\n\nEstado actual:  1\nPosibles acciones:  [0 2 8]\nPesos de esas acciones:  [0 0 0]\nPosición del mejor movimiento, en base a su peso:  0\nMejor acción:  0\n\nEstado actual:  0\nPosibles acciones:  [1 2 5 7 8]\nPesos de esas acciones:  [0 0 0 0 0]\nPosición del mejor movimiento, en base a su peso:  0\nMejor acción:  1\n\nEstado actual:  1\nPosibles acciones:  [0 2 8]\nPesos de esas acciones:  [0 0 0]\nPosición del mejor movimiento, en base a su peso:  0\nMejor acción:  0\n\nEstado actual:  0\nPosibles acciones:  [1 2 5 7 8]\nPesos de esas acciones:  [0 0 0 0 0]\nPosición del mejor movimiento, en base a su peso:  0\nMejor acción:  1\n\nEstado actual:  1\nPosibles acciones:  [0 2 8]\nPesos de esas acciones:  [0 0 0]\nPosición del mejor movimiento, en base a su peso:  0\nMejor acción:  0\nEl agente no está lo suficientemente entrenado y no encuentra la meta, finalizando recorrido.\nEstado inicial escogido:  5\nMeta escogida:  6\nGamma:  0.5\nEpisodios:  5\nEstado actual:  5\nPosibles acciones:  [0 7 8]\nAcción escogida al azar:  0\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 0 :  [1 2 5 7 8]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  316.66666666666663\n\n\n[(5, 0), (5, 7), (5, 8)]\nEstado actual:  0\nPosibles acciones:  [1 2 5 7 8]\nAcción escogida al azar:  5\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 5 :  [0 7 8]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  316.66666666666663\n\n\n[(5, 0), (5, 7), (5, 8), (0, 1), (0, 2), (0, 5), (0, 7), (0, 8)]\nEstado actual:  5\nPosibles acciones:  [0 7 8]\nAcción escogida al azar:  7\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 7 :  [0 3 4 5 8]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  316.66666666666663\n\n\n[(5, 0), (5, 7), (5, 8), (0, 1), (0, 2), (0, 5), (0, 7), (0, 8), (5, 0), (5, 7), (5, 8)]\nEstado actual:  7\nPosibles acciones:  [0 3 4 5 8]\nAcción escogida al azar:  8\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 8 :  [0 1 2 3 4 5 6 7]\nPeso en Q para esas futuras acciones:  [ 0  0  0 75  0  0  0  0]\nEl rendimiento de este episodio de entrenamiento ha sido:  341.33333333333337\n\n\n[(5, 0), (5, 7), (5, 8), (0, 1), (0, 2), (0, 5), (0, 7), (0, 8), (5, 0), (5, 7), (5, 8), (7, 0), (7, 3), (7, 4), (7, 5), (7, 8)]\nEstado actual:  8\nPosibles acciones:  [0 1 2 3 4 5 6 7]\nAcción escogida al azar:  0\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 0 :  [1 2 5 7 8]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  341.33333333333337\n\n\n[(5, 0), (5, 7), (5, 8), (0, 1), (0, 2), (0, 5), (0, 7), (0, 8), (5, 0), (5, 7), (5, 8), (7, 0), (7, 3), (7, 4), (7, 5), (7, 8), (8, 0), (8, 1), (8, 2), (8, 3), (8, 4), (8, 5), (8, 6), (8, 7)]\nEntrenamientos [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36.\n 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54.\n 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71. 72.\n 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86.]\nRendimientos [  0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.         100.         100.         100.\n 100.         100.         100.         100.         100.\n 166.66666667 166.66666667 216.66666667 216.66666667 216.66666667\n 216.66666667 216.66666667 216.66666667 216.66666667 216.66666667\n 216.66666667 216.66666667 216.66666667 216.66666667 216.66666667\n 216.66666667 216.66666667 216.66666667 216.66666667 216.66666667\n 266.66666667 266.66666667 266.66666667 266.66666667 266.66666667\n 316.66666667 316.66666667 316.66666667 316.66666667 341.33333333\n 341.33333333]\n[[  0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0]\n [  0   0   0  75   0   0   0   0   0]\n [  0   0   0   0   0   0 150   0   0]\n [  0   0   0  75   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0 100   0   0]\n [  0   0   0   0   0   0   0   0  37]\n [  0   0   0  75   0   0   0   0   0]]\nEstado inicial escogido:  5\nMeta escogida:  6\nGamma:  0.5\nEpisodios:  5\nEstado actual:  5\nPosibles acciones:  [0 7 8]\nAcción escogida al azar:  0\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 0 :  [1 2 5 7 8]\nPeso en Q para esas futuras acciones:  [0 0 0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  341.33333333333337\n\n\n[(5, 0), (5, 7), (5, 8)]\nEstado actual:  0\nPosibles acciones:  [1 2 5 7 8]\nAcción escogida al azar:  2\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 2 :  [0 1 3 6 8]\nPeso en Q para esas futuras acciones:  [ 0  0 75  0  0]\nEl rendimiento de este episodio de entrenamiento ha sido:  366.0\n\n\n[(5, 0), (5, 7), (5, 8), (0, 1), (0, 2), (0, 5), (0, 7), (0, 8)]\nEstado actual:  2\nPosibles acciones:  [0 1 3 6 8]\nAcción escogida al azar:  6\nPeso en R de esa acción:  100\nAcciones que serán posibles cuando esté en 6 :  [2 3 6 8]\nPeso en Q para esas futuras acciones:  [  0   0 100   0]\nEl rendimiento de este episodio de entrenamiento ha sido:  466.0\n\n\n[(5, 0), (5, 7), (5, 8), (0, 1), (0, 2), (0, 5), (0, 7), (0, 8), (2, 0), (2, 1), (2, 3), (2, 6), (2, 8)]\nHemos alcanzado la meta antes de terminar las iteraciones.\nEntrenamientos [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36.\n 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54.\n 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71. 72.\n 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.]\nRendimientos [  0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.         100.         100.         100.\n 100.         100.         100.         100.         100.\n 166.66666667 166.66666667 216.66666667 216.66666667 216.66666667\n 216.66666667 216.66666667 216.66666667 216.66666667 216.66666667\n 216.66666667 216.66666667 216.66666667 216.66666667 216.66666667\n 216.66666667 216.66666667 216.66666667 216.66666667 216.66666667\n 266.66666667 266.66666667 266.66666667 266.66666667 266.66666667\n 316.66666667 316.66666667 316.66666667 316.66666667 341.33333333\n 341.33333333 341.33333333 366.         466.        ]\n[[  0   0  37   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0]\n [  0   0   0  75   0   0 150   0   0]\n [  0   0   0   0   0   0 150   0   0]\n [  0   0   0  75   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0 100   0   0]\n [  0   0   0   0   0   0   0   0  37]\n [  0   0   0  75   0   0   0   0   0]]\nEstado inicial escogido:  0\nMeta escogida:  6\nGamma:  0.5\nEpisodios:  5\nEstado actual:  0\nPosibles acciones:  [1 2 5 7 8]\nAcción escogida al azar:  5\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 5 :  [0 7 8]\nPeso en Q para esas futuras acciones:  [0 0 0]\nEl rendimiento de este episodio de entrenamiento ha sido:  466.0\n\n\n[(0, 1), (0, 2), (0, 5), (0, 7), (0, 8)]\nEstado actual:  5\nPosibles acciones:  [0 7 8]\nAcción escogida al azar:  8\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 8 :  [0 1 2 3 4 5 6 7]\nPeso en Q para esas futuras acciones:  [ 0  0  0 75  0  0  0  0]\nEl rendimiento de este episodio de entrenamiento ha sido:  490.66666666666663\n\n\n[(0, 1), (0, 2), (0, 5), (0, 7), (0, 8), (5, 0), (5, 7), (5, 8)]\nEstado actual:  8\nPosibles acciones:  [0 1 2 3 4 5 6 7]\nAcción escogida al azar:  6\nPeso en R de esa acción:  100\nAcciones que serán posibles cuando esté en 6 :  [2 3 6 8]\nPeso en Q para esas futuras acciones:  [  0   0 100   0]\nEl rendimiento de este episodio de entrenamiento ha sido:  590.6666666666666\n\n\n[(0, 1), (0, 2), (0, 5), (0, 7), (0, 8), (5, 0), (5, 7), (5, 8), (8, 0), (8, 1), (8, 2), (8, 3), (8, 4), (8, 5), (8, 6), (8, 7)]\nHemos alcanzado la meta antes de terminar las iteraciones.\nEntrenamientos [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36.\n 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54.\n 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71. 72.\n 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89. 90.\n 91. 92.]\nRendimientos [  0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.         100.         100.         100.\n 100.         100.         100.         100.         100.\n 166.66666667 166.66666667 216.66666667 216.66666667 216.66666667\n 216.66666667 216.66666667 216.66666667 216.66666667 216.66666667\n 216.66666667 216.66666667 216.66666667 216.66666667 216.66666667\n 216.66666667 216.66666667 216.66666667 216.66666667 216.66666667\n 266.66666667 266.66666667 266.66666667 266.66666667 266.66666667\n 316.66666667 316.66666667 316.66666667 316.66666667 341.33333333\n 341.33333333 341.33333333 366.         466.         466.\n 490.66666667 590.66666667]\n[[  0   0  37   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0]\n [  0   0   0  75   0   0 150   0   0]\n [  0   0   0   0   0   0 150   0   0]\n [  0   0   0  75   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0  37]\n [  0   0   0   0   0   0 100   0   0]\n [  0   0   0   0   0   0   0   0  37]\n [  0   0   0  75   0   0 150   0   0]]\nEstado inicial escogido:  6\nMeta escogida:  6\nGamma:  0.5\nEpisodios:  5\nEstado actual:  6\nPosibles acciones:  [2 3 6 8]\nAcción escogida al azar:  6\nPeso en R de esa acción:  100\nAcciones que serán posibles cuando esté en 6 :  [2 3 6 8]\nPeso en Q para esas futuras acciones:  [  0   0 100   0]\nEl rendimiento de este episodio de entrenamiento ha sido:  624.0\n\n\n[(6, 2), (6, 3), (6, 6), (6, 8)]\nHemos alcanzado la meta antes de terminar las iteraciones.\nEntrenamientos [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36.\n 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54.\n 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71. 72.\n 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89. 90.\n 91. 92. 93.]\nRendimientos [  0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.         100.         100.         100.\n 100.         100.         100.         100.         100.\n 166.66666667 166.66666667 216.66666667 216.66666667 216.66666667\n 216.66666667 216.66666667 216.66666667 216.66666667 216.66666667\n 216.66666667 216.66666667 216.66666667 216.66666667 216.66666667\n 216.66666667 216.66666667 216.66666667 216.66666667 216.66666667\n 266.66666667 266.66666667 266.66666667 266.66666667 266.66666667\n 316.66666667 316.66666667 316.66666667 316.66666667 341.33333333\n 341.33333333 341.33333333 366.         466.         466.\n 490.66666667 590.66666667 624.        ]\n[[  0   0  37   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0]\n [  0   0   0  75   0   0 150   0   0]\n [  0   0   0   0   0   0 150   0   0]\n [  0   0   0  75   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0  37]\n [  0   0   0   0   0   0 150   0   0]\n [  0   0   0   0   0   0   0   0  37]\n [  0   0   0  75   0   0 150   0   0]]\nEstado inicial escogido:  0\nMeta escogida:  6\nGamma:  0.5\nEpisodios:  5\nEstado actual:  0\nPosibles acciones:  [1 2 5 7 8]\nAcción escogida al azar:  8\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 8 :  [0 1 2 3 4 5 6 7]\nPeso en Q para esas futuras acciones:  [  0   0   0  75   0   0 150   0]\nEl rendimiento de este episodio de entrenamiento ha sido:  674.0\n\n\n[(0, 1), (0, 2), (0, 5), (0, 7), (0, 8)]\nEstado actual:  8\nPosibles acciones:  [0 1 2 3 4 5 6 7]\nAcción escogida al azar:  3\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 3 :  [2 4 6 7 8]\nPeso en Q para esas futuras acciones:  [  0   0 150   0   0]\nEl rendimiento de este episodio de entrenamiento ha sido:  674.0\n\n\n[(0, 1), (0, 2), (0, 5), (0, 7), (0, 8), (8, 0), (8, 1), (8, 2), (8, 3), (8, 4), (8, 5), (8, 6), (8, 7)]\nEstado actual:  3\nPosibles acciones:  [2 4 6 7 8]\nAcción escogida al azar:  4\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 4 :  [3 7 8]\nPeso en Q para esas futuras acciones:  [75  0  0]\nEl rendimiento de este episodio de entrenamiento ha sido:  698.6666666666666\n\n\n[(0, 1), (0, 2), (0, 5), (0, 7), (0, 8), (8, 0), (8, 1), (8, 2), (8, 3), (8, 4), (8, 5), (8, 6), (8, 7), (3, 2), (3, 4), (3, 6), (3, 7), (3, 8)]\nEstado actual:  4\nPosibles acciones:  [3 7 8]\nAcción escogida al azar:  3\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 3 :  [2 4 6 7 8]\nPeso en Q para esas futuras acciones:  [  0  37 150   0   0]\nEl rendimiento de este episodio de entrenamiento ha sido:  698.6666666666666\n\n\n[(0, 1), (0, 2), (0, 5), (0, 7), (0, 8), (8, 0), (8, 1), (8, 2), (8, 3), (8, 4), (8, 5), (8, 6), (8, 7), (3, 2), (3, 4), (3, 6), (3, 7), (3, 8), (4, 3), (4, 7), (4, 8)]\nEstado actual:  3\nPosibles acciones:  [2 4 6 7 8]\nAcción escogida al azar:  4\nPeso en R de esa acción:  0\nAcciones que serán posibles cuando esté en 4 :  [3 7 8]\nPeso en Q para esas futuras acciones:  [75  0  0]\nEl rendimiento de este episodio de entrenamiento ha sido:  698.6666666666666\n\n\n[(0, 1), (0, 2), (0, 5), (0, 7), (0, 8), (8, 0), (8, 1), (8, 2), (8, 3), (8, 4), (8, 5), (8, 6), (8, 7), (3, 2), (3, 4), (3, 6), (3, 7), (3, 8), (4, 3), (4, 7), (4, 8), (3, 2), (3, 4), (3, 6), (3, 7), (3, 8)]\nEntrenamientos [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36.\n 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54.\n 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71. 72.\n 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89. 90.\n 91. 92. 93. 94. 95. 96. 97. 98.]\nRendimientos [  0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.           0.           0.           0.\n   0.           0.         100.         100.         100.\n 100.         100.         100.         100.         100.\n 166.66666667 166.66666667 216.66666667 216.66666667 216.66666667\n 216.66666667 216.66666667 216.66666667 216.66666667 216.66666667\n 216.66666667 216.66666667 216.66666667 216.66666667 216.66666667\n 216.66666667 216.66666667 216.66666667 216.66666667 216.66666667\n 266.66666667 266.66666667 266.66666667 266.66666667 266.66666667\n 316.66666667 316.66666667 316.66666667 316.66666667 341.33333333\n 341.33333333 341.33333333 366.         466.         466.\n 490.66666667 590.66666667 624.         674.         674.\n 698.66666667 698.66666667 698.66666667]\n[[  0   0  37   0   0   0   0   0  75]\n [  0   0   0   0   0   0   0   0   0]\n [  0   0   0  75   0   0 150   0   0]\n [  0   0   0   0  37   0 150   0   0]\n [  0   0   0  75   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0  37]\n [  0   0   0   0   0   0 150   0   0]\n [  0   0   0   0   0   0   0   0  37]\n [  0   0   0  75   0   0 150   0   0]]\nEl estado incial es:  0\nLa meta es:  6\n\nEstado actual:  0\nPosibles acciones:  [1 2 5 7 8]\nPesos de esas acciones:  [ 0 37  0  0 75]\nPosición del mejor movimiento, en base a su peso:  4\nMejor acción:  8\n\nEstado actual:  8\nPosibles acciones:  [0 1 2 3 4 5 6 7]\nPesos de esas acciones:  [  0   0   0  75   0   0 150   0]\nPosición del mejor movimiento, en base a su peso:  6\nMejor acción:  6\nHicieron falta 2 movimientos para llegar a la meta.\n\n"
    }
   ],
   "source": [
    "from tkinter import *\n",
    "from tkinter import messagebox as mb \n",
    "from random import sample   \n",
    "import numpy as np\n",
    "#Iniciamos la ventana Principal\n",
    "ventana = Tk()\n",
    "ventana.title(\"Aprendizaje por Refuerzo\")\n",
    "ventana.geometry('1200x1200')\n",
    "#ventana.configure(background = 'dark turquoise')\n",
    "\n",
    "#Añadimos las entradas de texto de Nº de Filas y Columnas\n",
    "filas = Label(ventana,text=\"Numero de Filas\")\n",
    "entry_fila = Entry(ventana, width=5)\n",
    "filas.grid(column=0,row=0)\n",
    "entry_fila.grid(column=1,row=0)\n",
    "\n",
    "columnas = Label(ventana,text=\"Numero de Columnas\")\n",
    "entry_columna = Entry(ventana, width=5)\n",
    "columnas.grid(column=0,row=3)\n",
    "entry_columna.grid(column=1,row=3)\n",
    "\n",
    "inic = Label(ventana,text=\"Posicion de inicio\")\n",
    "entry_inic = Entry(ventana, width=5)\n",
    "entry_inic.insert(END,0)\n",
    "inic.grid(column=2,row=0)\n",
    "entry_inic.grid(column=3,row=0)\n",
    "\n",
    "fin = Label(ventana,text=\"Objetivo\")\n",
    "entry_fin = Entry(ventana, width=5)\n",
    "entry_fin.insert(END,6)\n",
    "fin.grid(column=2,row=3)\n",
    "entry_fin.grid(column=3,row=3)\n",
    "\n",
    "gamma = Label(ventana,text=\"Valor Gamma\")\n",
    "entry_gamma = Entry(ventana, width=5)\n",
    "entry_gamma.insert(END,0.5)\n",
    "gamma.grid(column=4,row=0)\n",
    "entry_gamma.grid(column=5,row=0)\n",
    "\n",
    "iteraciones = Label(ventana,text=\"Numero de Iteraciones\")\n",
    "entry_iteraciones = Entry(ventana, width=5)\n",
    "entry_iteraciones.insert(END,3)\n",
    "iteraciones.grid(column=4,row=3)\n",
    "entry_iteraciones.grid(column=5,row=3)\n",
    "\n",
    "epsilon = Label(ventana,text=\"Valor Epsilon\")\n",
    "entry_epsilon = Entry(ventana, width=10)\n",
    "entry_epsilon.insert(END,0.8)\n",
    "epsilon.grid(column=6,row=0)\n",
    "entry_epsilon.grid(column=7,row=0)\n",
    "\n",
    "alpha = Label(ventana,text=\"Valor Alpha\")\n",
    "entry_alpha = Entry(ventana, width=5)\n",
    "entry_alpha.insert(END,0.9)\n",
    "alpha.grid(column=6,row=3)\n",
    "entry_alpha.grid(column=7,row=3)\n",
    "\n",
    "\n",
    "#Clase Matriz\n",
    "class Matrix():\n",
    "    def __init__(self,celdas,pos,valores,start,nueva_lista):\n",
    "        self.celdas=celdas[:]\n",
    "        self.pos=pos[:]\n",
    "        self.valores=valores[:]\n",
    "        self.start=start[:]\n",
    "        self.nueva_lista=nueva_lista[:]\n",
    "\n",
    "#Clase Recorrido (muestra datos de recorrido final)\n",
    "class Recorrido():\n",
    "    def __init__(self,label):\n",
    "        self.label=label[:]\n",
    "        \n",
    "recorrido = Recorrido([])\n",
    "\n",
    "#Clase EntrenamientoAlternativo\n",
    "class Alternative():\n",
    "    def __init__(self,label,epsilon,alfa):\n",
    "        self.label=label[:]\n",
    "        self.epsilon=epsilon[:]\n",
    "        self.alfa=alfa[:]\n",
    "\n",
    "alternative = Alternative([],[],[])\n",
    "\n",
    "#Clase datos (entrenamientos y rendimientos)\n",
    "class Datos:\n",
    "    def __init__(self,entrenamientos, rendimientos):\n",
    "        self.entrenamientos=entrenamientos[:]\n",
    "        self.rendimientos=rendimientos[:]\n",
    "datos = Datos([],[])\n",
    "\n",
    "#m1 instanciacion de matriz vacía\n",
    "m1 = Matrix([],[],[],[],[])\n",
    "m2 = Matrix([],[],[],[],[])\n",
    "\n",
    "\n",
    "def aleatorizar(e,n,valores):\n",
    "    \n",
    "    random = sample([x for x in range(0,n)],n)[0]\n",
    "    while(random in valores):\n",
    "        random = sample([x for x in range(0,n)],n)[0]\n",
    "    valores.append(random)\n",
    "    e.insert(END,random)\n",
    "\n",
    "\n",
    "def generarMatriz(m1):\n",
    "\n",
    "#En el caso de que m1 sea una matriz ya iniciada, esta tendrá celdas, valores, y posiciones que hay que eliminar previamente    \n",
    "    celdas = m1.celdas\n",
    "    pos = m1.pos\n",
    "    valores = m1.valores\n",
    "    nueva_lista=m1.nueva_lista\n",
    "    start=m1.start\n",
    "\n",
    "    del m1.pos[:]\n",
    "    del m1.valores[:]\n",
    "    for b in start:\n",
    "        b.destroy()\n",
    "    del m1.nueva_lista[:]\n",
    "    for c in celdas:\n",
    "        c.destroy()\n",
    "\n",
    "#Una vez eliminados validamos las entradas de nfilas y ncolumnas\n",
    "    validate_tam(entry_fila.get(),entry_columna.get())\n",
    "    n_filas=int(entry_fila.get())\n",
    "    n_columnas= int(entry_columna.get())\n",
    "    total = n_filas * n_columnas\n",
    "\n",
    "    # Creamos la tabla de entradas\n",
    "    for row in range(n_filas):\n",
    "        for column in range(n_columnas):\n",
    "            index = (row, column)\n",
    "            e = Entry(ventana, width=10)\n",
    "            aleatorizar(e,total,valores)\n",
    "            e.grid(row=row+10, column=column, stick=\"nsew\")\n",
    "            celdas.append(e)\n",
    "            pos.append(index)\n",
    "\n",
    "            \n",
    "    m1.celdas= celdas\n",
    "    m1.pos = pos\n",
    "    m1.valores= valores\n",
    "    result = zip(pos,valores)\n",
    "    result_set = set(result)\n",
    "    inic_QR = Button(ventana, text= \"Generar Q y R\", command=lambda: \n",
    "    iniciar_tablero(n_filas, start,m2))\n",
    "    inic_QR.grid(column=2, row=4)\n",
    "    #AQUÍ SE HACE LA MAGIA\n",
    "\n",
    "def validate_tam(n_filas,n_columnas):\n",
    "    try:\n",
    "        f= int(n_filas)\n",
    "        c= int(n_columnas)\n",
    "    except ValueError:\n",
    "        mb.showerror(\"Error\",\"Debe introducir un valor entero como numero de filas y columnas\")\n",
    "    \n",
    "    if(int(n_filas) == 0 or (n_columnas) == 0):\n",
    "        mb.showerror(\"Error\",\"Ni la fila ni la columna puede ser de tamaño 0\")\n",
    "    \n",
    "#Creamos la matriz según las entradas\n",
    "btn = Button(ventana, text=\"Generar Matriz\", command=lambda: generarMatriz(m1))\n",
    "btn.grid(column=0, row=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ventana.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264biteb45e1f164454beda4749fe81198b568"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}